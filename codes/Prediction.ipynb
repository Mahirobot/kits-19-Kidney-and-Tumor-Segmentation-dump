{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Prediction.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP4DfAFP+XHcXQ/MGhKNbJ6"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"g76SIZ_FlCog"},"source":["import tensorflow as tf\r\n","import os\r\n","e = \"No gpu\"\r\n","gpus = tf.config.experimental.list_physical_devices('GPU')\r\n","if gpus:\r\n","  try:\r\n","    tf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=6148)])\r\n","  except RuntimeError as e:\r\n","    print(e)\r\n","\r\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2_N5PS4wlKjY"},"source":["import miscnn\r\n","\r\n","# Creating a Data I/O interface for kidney tumor CT scans in NIfTI format\r\n","from miscnn.data_loading.interfaces import NIFTI_interface\r\n","interface = NIFTI_interface(pattern=\"case_00[0-9]*\", channels=1, classes=3)\r\n","data_path = \"/media/rami/New Volume/kits19/data_resized/\"\r\n","data_io = miscnn.Data_IO(interface, data_path)\r\n","sample_list = data_io.get_indiceslist()\r\n","sample_list.sort()\r\n","print(\"All samples: \" + str(sample_list))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RyrhPzi0lnT_"},"source":["# Data Augmentation\r\n","from miscnn.processing.data_augmentation import Data_Augmentation\r\n","\r\n","# Configurations\r\n","data_aug = Data_Augmentation(cycles=2, scaling=True, rotations=True, elastic_deform=True, mirror=True,\r\n","                             brightness=True, contrast=True, gamma=True, gaussian_noise=True)\r\n","\r\n","#Subfunctions\r\n","from miscnn.processing.subfunctions.normalization import Normalization\r\n","from miscnn.processing.subfunctions.clipping import Clipping\r\n","from miscnn.processing.subfunctions.resampling import Resampling\r\n","\r\n","# Pixel value normalization Subfunction through Z-Score \r\n","sf_normalize = Normalization(mode='z-score')\r\n","# Clipping Subfunction between -79 and 304\r\n","sf_clipping = Clipping(min=-79, max=304)\r\n","# Resampling Subfunction to voxel spacing 3.22 x 1.62 x 1.62\r\n","sf_resample = Resampling((3.22, 1.62, 1.62))\r\n","\r\n","# Creating the list for Subfunctions\r\n","subfunctions = [sf_resample, sf_clipping, sf_normalize]\r\n","\r\n","# Preprocessor\r\n","from miscnn.processing.preprocessor import Preprocessor\r\n","data = data_io\r\n","\r\n","# Configure the Preprocessor class\r\n","pp = Preprocessor(data, data_aug=data_aug, batch_size=1, subfunctions=subfunctions, prepare_subfunctions=True, \r\n","                  prepare_batches=False, analysis=\"patchwise-crop\", patch_shape=(80, 80, 80),\r\n","                  use_multiprocessing=True)\r\n","\r\n","# Patch overlap for predictions\r\n","pp.patchwise_overlap = (40, 40, 40)\r\n","\r\n","# Network Model - Standard 3D Unet\r\n","from miscnn.neural_network.model import Neural_Network\r\n","#Specify loss functions to import\r\n","from miscnn.neural_network.metrics import dice_soft, dice_crossentropy, focal_tversky, background, kidney, tumor, log_cosh_dice_loss, dice_soft_loss\r\n","from miscnn.neural_network.architecture.unet.standard import Architecture\r\n","\r\n","unet_standard = Architecture()\r\n","# Create the Neural Network model\r\n","model = Neural_Network(preprocessor=pp, architecture=unet_standard ,loss= dice_soft_loss, metrics=[dice_soft, dice_crossentropy, background, kidney, tumor],\r\n","                       batch_queue_size=1, workers=3, learninig_rate=0.0001)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gEVFPYHVl0Dg"},"source":["# Excluding suspious samples from data set\r\n","del sample_list[133]\r\n","del sample_list[125]\r\n","del sample_list[68]\r\n","del sample_list[37]\r\n","del sample_list[23]\r\n","del sample_list[15]\r\n","\r\n","#Create train sample\r\n","train_samples = sample_list[0:160]\r\n","# Create the validation sample ID list\r\n","validation_samples = sample_list[160:204]\r\n","# Create the test sample ID list\r\n","test_samples = sample_list[204:294]\r\n","# Output validation samples\r\n","print(\"Validation samples: \" + str(validation_samples))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9fnWkg6ImRTg"},"source":["**Prediction**"]},{"cell_type":"code","metadata":{"id":"A_9QN91Sl77B"},"source":["model.reset_weights()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R7WxA0SFl9in"},"source":["model.load('3DUnet_dice_soft.hdf5')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lQNahOUHl_SB"},"source":["model.predict(validation_samples)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PghD15pNmBqB"},"source":["from evaluation.kits19_evaluation import kits19_evaluate\r\n","l = [int(x[-3:]) for x in validation_samples] \r\n","pred_path = \"/media/rami/New Volume/kits19/MIScnn/Pred_Results/dice_loss\"\r\n","log_path = \"/media/rami/New Volume/kits19/MIScnn/\"\r\n","\r\n","kits19_evaluate(l, pred_path, log_path)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"657ER_jImOQt"},"source":["**Visualize**"]},{"cell_type":"code","metadata":{"id":"Hb7VElqmmJP4"},"source":["import shutil\r\n","import math\r\n","import os\r\n","from tqdm import tqdm\r\n","import numpy as np \r\n","import pandas as pd\r\n","import os\r\n","import scipy.ndimage\r\n","import nibabel as nib\r\n","import matplotlib.pyplot as plt\r\n","from numpy.random import rand\r\n","from skimage import measure, morphology\r\n","from scipy.ndimage import zoom\r\n","import cv2\r\n","import tensorflow as tf\r\n","\r\n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NXp6Iqy3mLqw"},"source":["def hu_to_grayscale(volume, hu_min = -512, hu_max = 512):\r\n","    # Clip at max and min values if specified\r\n","    if hu_min is not None or hu_max is not None:\r\n","        volume = np.clip(volume, hu_min, hu_max)\r\n","\r\n","    # Scale to values between 0 and 1\r\n","    mxval = np.max(volume)\r\n","    mnval = np.min(volume)\r\n","    im_volume = (volume - mnval)/max(mxval - mnval, 1e-3)\r\n","\r\n","    # Return values scaled to 0-255 range, but *not cast to uint8*\r\n","    # Repeat three times to make compatible with color overlay\r\n","    im_volume = 255*im_volume\r\n","    return np.stack((im_volume, im_volume, im_volume), axis=-1)\r\n","\r\n","def class_to_color(segmentation, k_color = [255, 0, 0], t_color = [0, 0, 255]):\r\n","    # initialize output to zeros\r\n","    shp = segmentation.shape\r\n","    seg_color = np.zeros((shp[0], shp[1], shp[2], 3), dtype=np.float32)\r\n","\r\n","    # set output to appropriate color at each location\r\n","    seg_color[np.equal(segmentation,1)] = k_color\r\n","    seg_color[np.equal(segmentation,2)] = t_color\r\n","    return seg_color\r\n","\r\n","def overlay(volume_ims, segmentation_ims, segmentation, alpha):\r\n","    # Get binary array for places where an ROI lives\r\n","    segbin = np.greater(segmentation, 0)\r\n","    repeated_segbin = np.stack((segbin, segbin, segbin), axis=-1)\r\n","    # Weighted sum where there's a value to overlay\r\n","    overlayed = np.where(\r\n","        repeated_segbin,\r\n","        np.round(alpha*segmentation_ims+(1-alpha)*volume_ims).astype(np.uint8),\r\n","        np.round(volume_ims).astype(np.uint8)\r\n","    )\r\n","    return overlayed\r\n","\r\n","#Visualize a case after resizing\r\n","def Visualize(case, rows, columns, width = 299, height = 299):\r\n","    cid = f'{case:05}'\r\n","    seg_best = f\"/media/rami/New Volume/kits19/MIScnn/Pred_Results/weighted_dice\"\r\n","    seg_worst = f\"/media/rami/New Volume/kits19/MIScnn/Pred_Results/dice_loss\"\r\n","    orig = f\"/media/rami/New Volume/kits19/data_resized/case_{cid}/\"\r\n","    img = f\"/media/rami/New Volume/kits19/data_resized/case_{cid}/\"\r\n","    img = os.path.join(img, 'imaging.nii.gz') \r\n","    seg_best = os.path.join(seg_best, f'case_{cid}.nii.gz')\r\n","    seg_worst = os.path.join(seg_worst, f'case_{cid}.nii.gz')\r\n","    \r\n","    orig = os.path.join(orig, 'segmentation.nii.gz') \r\n","    img = nib.load(img)\r\n","    \r\n","    seg_best = nib.load(seg_best)\r\n","    seg_worst = nib.load(seg_worst)\r\n","    orig = nib.load(orig)\r\n","    \r\n","    img = img.get_fdata()\r\n","    seg_best = seg_best.get_fdata()\r\n","    seg_worst = seg_worst.get_fdata()\r\n","    orig = orig.get_fdata()\r\n","    seg_best = seg_best.astype(np.float32)\r\n","    seg_worst = seg_worst.astype(np.float32)\r\n","    orig = orig.astype(np.float32)\r\n","    img_siz = hu_to_grayscale(img, -width, height)\r\n","    seg_best = class_to_color(seg_best, [255, 0, 0], [0, 0, 255])\r\n","    seg_worst = class_to_color(seg_worst, [255, 0, 0], [0, 0, 255])\r\n","    orig_siz = class_to_color(orig, [255, 0, 0], [0, 0, 255])\r\n","    \r\n","    plt.subplots(rows, columns,figsize=(15, 15))\r\n","    x = 85\r\n","    plt.subplot(rows, columns, 2)\r\n","    image_pred = overlay(img_siz, seg_best, img, 0.3)\r\n","    plt.imshow(image_pred[x], cmap='gray')\r\n","    plt.subplot(rows, columns, 1)\r\n","    image_orig = overlay(img_siz, orig_siz, img, 0.3)\r\n","    plt.imshow(image_orig[x], cmap='gray')\r\n","    plt.show()\r\n","    \r\n","    \r\n","rows = 1\r\n","columns = 2\r\n","case = 189\r\n","Visualize(case, rows, columns, 256, 256)"],"execution_count":null,"outputs":[]}]}